@misc{vaswani_2017_attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  month = {06},
  title = {Attention Is All You Need},
  url = {https://arxiv.org/abs/1706.03762},
  year = {2017},
  organization = {arXiv.org}
}

@article{bubeck_2023_sparks,
  author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  month = {03},
  title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  url = {https://arxiv.org/abs/2303.12712},
  year = {2023},
  journal = {arXiv:2303.12712 [cs]}
}

@article{brown_2020_language,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month = {05},
  title = {Language Models are Few-Shot Learners},
  url = {https://arxiv.org/abs/2005.14165},
  year = {2020},
  journal = {arxiv.org}
}

@misc{garg_2023_what,
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  month = {01},
  title = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  doi = {10.48550/arXiv.2208.01066},
  url = {https://arxiv.org/abs/2208.01066},
  year = {2023},
  organization = {arXiv.org}
}

@book{thrun_1998_learning,
  author = {Thrun, Sebastian},
  editor = {Pratt, Lorien},
  publisher = {Springer US},
  title = {Learning to Learn},
  doi = {10.1007/978-1-4615-5529-2},
  year = {1998}
}

@misc{vonoswald_2023_transformers,
  author = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, João and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  month = {05},
  title = {Transformers learn in-context by gradient descent},
  doi = {10.48550/arXiv.2212.07677},
  url = {https://arxiv.org/abs/2212.07677},
  urldate = {2023-08-31},
  year = {2023},
  organization = {arXiv.org}
}

@article{power_2022_grokking,
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  month = {01},
  title = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  url = {https://arxiv.org/abs/2201.02177},
  year = {2022},
  journal = {arXiv:2201.02177 [cs]}
}

@article{nanda_2023_progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  month = {01},
  title = {Progress measures for grokking via mechanistic interpretability},
  doi = {10.48550/arxiv.2301.05217},
  year = {2023},
  journal = {}
}

@misc{conmy_2023_towards,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
  month = {10},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  doi = {10.48550/arXiv.2304.14997},
  url = {https://arxiv.org/abs/2304.14997},
  urldate = {2024-04-10},
  year = {2023},
  organization = {arXiv.org}
}

@misc{geiger_2021_causal,
  author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
  month = {10},
  title = {Causal Abstractions of Neural Networks},
  doi = {10.48550/arXiv.2106.02997},
  url = {https://arxiv.org/abs/2106.02997},
  urldate = {2024-04-10},
  year = {2021},
  organization = {arXiv.org}
}

@misc{zhang_2023_towards,
  author = {Zhang, Fred and Nanda, Neel},
  month = {10},
  title = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  url = {https://openreview.net/forum?id=Hf17y6u9BC},
  urldate = {2024-04-11},
  year = {2023},
  organization = {openreview.net}
}

@misc{olsson_2022_incontext,
  author = {Olsson, Catherine and Nanda, Neel},
  month = {03},
  title = {In-context Learning and Induction Heads},
  url = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
  year = {2022},
  organization = {transformer-circuits.pub}
}

@misc{mcdougall_2023_copy,
  author = {McDougall, Callum and Conmy, Arthur and Rushing, Cody and McGrath, Thomas and Nanda, Neel},
  month = {10},
  title = {Copy Suppression: Comprehensively Understanding an Attention Head},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=GLnX3MkAAAAJ&citation_for_view=GLnX3MkAAAAJ:9ZlFYXVOiuMC},
  urldate = {2024-04-11},
  year = {2023},
  organization = {arXiv.org}
}

@misc{elhage_2021_a,
  author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine},
  month = {12},
  title = {A Mathematical Framework for Transformer Circuits},
  url = {https://transformer-circuits.pub/2021/framework/index.html},
  year = {2021},
  organization = {transformer-circuits.pub}
}

@article{bengio_2009_curriculum,
  author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
  title = {Curriculum learning},
  doi = {10.1145/1553374.1553380},
  year = {2009},
  journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09}
}

@misc{alain_2018_understanding,
  author = {Alain, Guillaume and Bengio, Yoshua},
  month = {11},
  title = {Understanding intermediate layers using linear classifier probes},
  doi = {10.48550/arXiv.1610.01644},
  url = {https://arxiv.org/abs/1610.01644},
  urldate = {2024-04-23},
  year = {2018},
  organization = {arXiv.org}
}

@article{brown_2020_language,
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month = {05},
  title = {Language Models are Few-Shot Learners},
  url = {https://arxiv.org/abs/2005.14165},
  year = {2020},
  journal = {arxiv.org}
}